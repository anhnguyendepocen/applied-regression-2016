%% beamer/knitr slides 
%% for Statistical Modeling and Data Visualization course @ UMass
%% Nicholas Reich: nick [at] schoolph.umass.edu


\documentclass[table]{beamer}


\input{../../slide-includes/standard-knitr-beamer-preamble}

%        The following variables are assumed by the standard preamble:
%        Global variable containing module name:

\title{Multiple Linear Regression: \\ Parameter Inference}
%	Global variable containing module shortname:
%		(Currently unused, may be used in future.)
\newcommand{\ModuleShortname}{multRegression}
%	Global variable containing author name:
\author{Nicholas G Reich, Jeff Goldsmith}
%	Global variable containing text of license terms:
\newcommand{\LicenseText}{Made available under the Creative Commons Attribution-ShareAlike 3.0 Unported License: http://creativecommons.org/licenses/by-sa/3.0/deed.en\textunderscore US }
%	Instructor: optional, can leave blank.
%		Recommended format: {Instructor: Jane Doe}
\newcommand{\Instructor}{}
%	Course: optional, can leave blank.
%		Recommended format: {Course: Biostatistics 101}
\newcommand{\Course}{}


\input{../../slide-includes/shortcuts}
\usepackage{bbm}

\hypersetup{colorlinks,linkcolor=,urlcolor=MainColor}


%	******	Document body begins here	**********************

\begin{document}

%	Title page
\begin{frame}[plain]
	\titlepage
\end{frame}

%	******	Everything through the above line must be placed at
%		the top of any TeX file using the statsTeachR standard
%		beamer preamble. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% acutal slides
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Today's Lecture}

\bi
    \myitem Sampling distribution of $\hat{\bbeta}$
    \myitem Confidence intervals
    \myitem Hypothesis tests for individual coefficients
    \myitem Multiple testing
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Circle of Life}

\begin{figure}[t]
    \includegraphics[width=.8\textwidth]{../../slide-includes/CircleOfLife.pdf}  
\end{figure}

\end{frame} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Statistical inference}

\bi
	\myitem We have LSEs $\hat{\beta}_0, \hat{\beta}_1, \ldots$; we want to know what this tells us about $\beta_0, \beta_1, \ldots$.
	\myitem Two basic tools are confidence intervals and hypothesis tests
	\bi
		\item Confidence intervals provide a plausible range of values for the parameter of interest based on the observed data
		\item Hypothesis tests ask how probable are the data we gathered under a null hypothesis about the data generating distribution
	\ei
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Motivation}

<<loadData, echo=FALSE, message=FALSE>>=
dat <- read.table("../lecture5-mlr-estimation-formulation/lungc.txt", header=TRUE)
require(ggplot2)
theme_set(theme_bw())
opts_chunk$set(size = 'footnotesize')
options(width=60)
@


How can we draw {\em \bf inference} about each of these parameters and relationships that our model is encoding?

\small
<<lungMLRCategorical, tidy=FALSE>>=
mlr1 <- lm(disease ~ airqual + crowding + nutrition + smoking, 
           data=dat)
summary(mlr1)$coef
@
 
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Motivation}

\bi
        \myitem Can we say anything about whether the effect of \texttt{airquality} is ``significant" after adjusting for other variables?
        \myitem Can we say whether adding \texttt{airquality} improves the fit of our model?
	\myitem Can we compare this model to a model with \texttt{crowding}, \texttt{nutrition} and \texttt{smoking}?
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Sampling distribution}

If our usual assumptions are satisfied and $\epsilon \stackrel{iid}{\sim} \N{0}{\sigma^2}$ then 
$$\hat{\bbeta} \sim \N{\bbeta}{\sigma^2 (\bX^{T}\bX)^{-1}}.$$
$$\hat{\beta}_j \sim \N{\bbeta}{\sigma^2 (\bX^{T}\bX)_{jj}^{-1}}.$$

\bi
        \myitem This will be used later for inference.
	\myitem Even without Normal errors, asymptotic Normality of LSEs is possible under reasonable assumptions.
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Sampling distribution}

For real data we have to estimate $\sigma^2$ as well as $\bbeta$.
\bi
	\myitem Recall our estimate of the error variance is 
		$$\hat{\sigma^2} = \frac{RSS}{n-p-1} = \frac{\sum_i (y_i - \hat{y}_i)^2}{n-p-1}$$
	\myitem With Normally distributed errors, it can be shown that 
		$$(n-p-1)\frac{\hat{\sigma^2}}{\sigma^2} \sim \chi^{2}_{n-p-1}$$
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Testing procedure}

Calculate the probability of the observed data (or more extreme data) under a null hypothesis.
\bi
	\myitem Often $H_{0}: \beta_j = 0$ and $H_{a}:\beta_j \neq 0$
	\myitem Set type I error rate $\alpha = P(\mbox{falsely rejecting a true null hypothesis})$ 
	\myitem Calculate a test statistic assuming the null hypothesis is true
	\myitem Compute a p-value = 
		$$P(\hat\beta_j \mbox{ as or more extreme as observed}|H_{0})$$
	\myitem Reject or fail to reject $H_0$
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Individual coefficients}

For individual coefficients
\bi
	\myitem We can use the test statistic 
		$$T = \frac{\hat{\beta}_j - \beta_j}{\widehat{se}(\hat{\beta}_j)}
		  = \frac{\hat{\beta}_j - \beta_j}{\sqrt{ \hat{\sigma}^2 (\bX^{T}\bX)_{jj}^{-1} }} \sim t_{n-p-1}$$
	\myitem For a two-sided test of size $\alpha$, we reject if 
		$$ |T| > t_{1-\alpha/2, n-p-1}$$
	\myitem The p-value gives $P(t_{n-p-1} > T_{obs} | H_{0})$
\ei
Note that $t$ is a symmetric distribution that converges to a Normal as $n-p-1$ increses.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Back to the example}

<<lungMLRTesting, tidy=FALSE, size="scriptsize">>=
summary(mlr1)
@
 
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Individual coefficients: CIs}

Alternatively, we can construct a confidence interval for $\beta_j$
\bi
        \myitem A confidence interval with coverage $(1-\alpha)$ is given by
		$$\hat{\beta}_j\pm t_{1-\alpha/2, n-p-1} \widehat{se}(\hat{\beta}_j)$$
	\myitem Assuming all the standard assumptions hold, 
		$$(1-\alpha) = P(LB < \beta_j < UB)$$
\ei

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Detour: confidence interval interpretations}

The semantics of confidence intervals are tricky!

\vspace{2em}

The technically correct interpretation of a (frequentist) confidence interval is: \\ {\em if the current experiment were repeated under similar conditions, we expect that $1-\alpha$\% of the time the confidence interval for a parameter would cover the true value of the parameter.}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Detour: confidence interval interpretations}

\begin{block}{Possible interpretations}
\bi
    \myitem ``There is a 95\% probability that this confidence interval contains the true value of the parameter.'' \\ WRONG! 
    \myitem ``We are 95\% confident that this interval contains the truth.'' \\ NOT VERY TECHNICALLY SPECIFIC, BUT NOT INCORRECT EITHER.
    \myitem ``The 95\% confidence interval for this parameter is (a, b).'' \\ COMMONLY USED, ASSUMES THE READER KNOWS HOW TO INTERPRET.
    \myitem ``With confidence coefficient .95, we estimate that the average change in Y per 1 unit increase of X lies somewhere between (a and b).'' \\ TECHNICALLY CORRECT, BUT NOT CLEAR WHAT CONF COEF IS.
\ei
\end{block}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Back to the example}

<<lungMLRConfInt, tidy=FALSE>>=
cbind(coef(mlr1), confint(mlr1))
@
 
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Confidence regions for multiple parameters}

If you want to draw inference about multiple parameters, it is better to look at them simultaneously.
 
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Plotting 2D confidence regions}

<<lungMLRConfIntEllipse, tidy=FALSE, fig.height=4, warning=FALSE>>=
library(ellipse)
plot(ellipse(mlr1,c(2,3)),type="l")
points(coef(mlr1)[2],coef(mlr1)[3], pch=18)
abline(v=c(confint(mlr1)[2,1], confint(mlr1)[2,2]), lty=2)
abline(h=c(confint(mlr1)[3,1], confint(mlr1)[3,2]), lty=2)
@
 
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Progress report}

\bi
    \myitem Sampling distribution of $\hat{\bbeta}$
    \myitem Confidence intervals
    \myitem Hypothesis tests for individual coefficients
    \myitem Multiple testing
\ei

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Multiple testing - preserving your Type I error rate}

\begin{figure}[t]
    \includegraphics[width=.4\textwidth]{significant.png}  
\end{figure}

\end{frame}

\begin{frame}{Multiple testing - preserving your Type I error rate}

\begin{figure}[t]
    \includegraphics[width=.8\textwidth]{significant-headline.png}  
\end{figure}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Inference about multiple coefficients}

Our model contains multiple parameters; often we want ask a question about multiple coefficients simultaneously. I.e. ``are any of these $k$ coefficients significantly different from 0?'' This is equivalent to performing multiple tests (or looking at confidence ellipses):
\beqa
	H_{01} : \beta_1 & = & 0\\
	H_{02} : \beta_2 & = & 0\\
	\vdots & = & \vdots \\	
	H_{0k} : \beta_k & = & 0\\
\eeqa
where each test has a size of $\alpha$
\bi
	\myitem For any individual test, $P(\mbox{reject } H_{0i} | H_{0i}) = \alpha$
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Inference about multiple coefficients}


For any individual test, $P(\mbox{reject } H_{0i} | H_{0i}) = \alpha$.

\vspace{2em}

But it DOES NOT FOLLOW that 
$$P(\mbox{reject \underline{at least one} } H_{0i} | \mbox{all } H_{0i} \mbox{are true}) = \alpha.$$
This is called the Family-wise error rate (FWER). Ignore it at your own peril!

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Family-wise error rate}

To calculate the FWER
\bi
	\myitem First note $P(\mbox{no rejections} | \mbox{all } H_{0i} \mbox{are true}) = (1-\alpha)^{k}$
	\myitem It follows that
	\beqa
		\mbox{FWER}  & = & P(\mbox{at least one rejection} | \mbox{all } H_{0i} \mbox{are true}) \\
                & = &  1 - (1-\alpha)^{k}
	\eeqa
\ei


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Family-wise error rate}

$$ \mbox{FWER} = 1 - (1-\alpha)^{k}$$ 
<<FWERPlot, fig.height=3.5, size="scriptsize">>=
alpha <- .05
k <- 1:100
FWER <- 1-(1-alpha)^k
qplot(k, FWER, geom="line") + geom_hline(yintercept = 1, lty=2)
@

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Addressing multiple comparisons}

Three general approaches
\bi
	\myitem Do nothing in a reasonable way
	\bi
		\item Don't trust scientifically implausible results
		\item Don't over-emphasize isolated findings
	\ei
	\myitem Correct for multiple comparisons
	\bi
		\item Often, use the Bonferroni correction and use $\alpha_i = \alpha /k$ for each test
		\item Thanks to the Bonferroni inequality, this gives an overall $FWER \leq \alpha$
	\ei
	\myitem Use a global test
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Global tests}

Compare a smaller ``null" model to a larger ``alternative" model
\bi
	\myitem Smaller model must be nested in the larger model
	\myitem That is, the smaller model must be a special case of the larger model
	\myitem For both models, the $RSS$ gives a general idea about how well the model is fitting
	\myitem In particular, something like 
		$$\frac{RSS_{S} - RSS_{L}}{RSS_{L}}$$
	compares the relative $RSS$ of the models
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Nested models}

\bi
	\myitem These models are nested:
	\beqa
		\mbox{Smaller} & = & \mbox{Regression of } Y \mbox{ on } X_1 \\
		\mbox{Larger} & = & \mbox{Regression of } Y \mbox{ on } X_1, X_2, X_3, X_4
	\eeqa
	\myitem These models are not:
	\beqa
		\mbox{Smaller} & = & \mbox{Regression of } Y \mbox{ on } X_2 \\
		\mbox{Larger} & = & \mbox{Regression of } Y \mbox{ on } X_1, X_3
	\eeqa
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Global $F$ tests}

\bi
	\myitem Compute the test statistic 
		$$F_{obs} = \frac{(RSS_{S} - RSS_{L})/(df_{S} - df_{L})}{RSS_{L}/df_{L}}$$
	\myitem If $H_{0}$ (the null model) is true, then $F_{obs} \sim F_{df_{S} - df_{L}, df_{L}}$
	\myitem Note $df_s = n -p_{S} -1$ and $df_{L} = n - p_{L} - 1$
	\myitem We reject the null hypothesis if the p-value is above $\alpha$, where
		$$\mbox{p-value} = P(F_{df_{S} - df_{L}, df_{L}} > F_{obs}) $$
	
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Global $F$ tests}

There are a couple of important special cases for the $F$ test
\bi
	\myitem The null model contains the intercept only
	\bi
		\item When people say ANOVA, this is often what they mean (although all $F$ tests are based on an analysis of variance)
	\ei
	\myitem The null model and the alternative model differ only by one term
	\bi
		\item Gives a way of testing for a single coefficient
		\item Turns out to be equivalent to a two-sided $t$-test: $t^{2}_{df_{L}} \sim F_{1, df_{L}}$
	\ei	
\ei

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Lung data: multiple coefficients simultaneously}

You can test multiple coefficients simultaneously using the $F$ test

\scriptsize
<<MLRmultcoef>>=
mlr_null <- lm(disease ~ nutrition, data=dat)
mlr1 <- lm(disease ~ nutrition+ airqual + crowding + smoking, data=dat)
anova(mlr_null, mlr1)
@

This test shows that airqual, crowding, and smoking together significantly improve the fit of our model (assuming model diagnostics look good). Further analyses may be warranted to determine which, if any, coefficients are not different from 0.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Lung data: single coefficient test}

The $F$ test is equivalent to the $t$ test when there's only one parameter of interest

<<MLRsinglecoef, size="scriptsize">>=
mlr_null <- lm(disease ~ nutrition, data=dat)
mlr1 <- lm(disease ~ nutrition + airqual, data=dat)
anova(mlr_null, mlr1)
summary(mlr1)$coef
@


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Today's Big Ideas}

\begin{block}{Basic parameter inference for multiple linear regression models} 
\bi
        \myitem How to determine ``significance'' of your covariates
        \myitem $F$ tests can control for multiple comparisons!
\ei
\end{block}

\href{http://nickreich.github.io/applied-regression-2016/assets/labs/lab3a-global-tests/global-tests.html}{Multiple testing activity!}

\end{frame}


\end{document}
