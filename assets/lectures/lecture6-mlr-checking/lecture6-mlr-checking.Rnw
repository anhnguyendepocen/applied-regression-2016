%% beamer/knitr slides 
%% for Statistical Modeling and Data Visualization course @ UMass
%% Nicholas Reich: nick [at] schoolph.umass.edu


\documentclass[table]{beamer}


\input{../../slide-includes/standard-knitr-beamer-preamble}

%        The following variables are assumed by the standard preamble:
%        Global variable containing module name:

\title{Multiple Linear Regression: \\ Model Checking and Diagnostics}
%	Global variable containing module shortname:
%		(Currently unused, may be used in future.)
\newcommand{\ModuleShortname}{multRegression}
%	Global variable containing author name:
\author{Nicholas G Reich, Jeff Goldsmith}
%	Global variable containing text of license terms:
\newcommand{\LicenseText}{Made available under the Creative Commons Attribution-ShareAlike 3.0 Unported License: http://creativecommons.org/licenses/by-sa/3.0/deed.en\textunderscore US }
%	Instructor: optional, can leave blank.
%		Recommended format: {Instructor: Jane Doe}
\newcommand{\Instructor}{}
%	Course: optional, can leave blank.
%		Recommended format: {Course: Biostatistics 101}
\newcommand{\Course}{}


\input{../../slide-includes/shortcuts}
\usepackage{bbm}

\hypersetup{colorlinks,linkcolor=,urlcolor=MainColor}


%	******	Document body begins here	**********************

\begin{document}

<<setup, include=FALSE>>=
library(knitr)
opts_chunk$set(fig.path='figure/beamer-',fig.align='center',fig.show='hold',size='footnotesize')
@


\begin{frame}[plain]
        \titlepage
\end{frame}

<<ggplot2, echo=FALSE, message=FALSE>>=
require(ggplot2)
theme_set(theme_bw())
@


\begin{frame}{Today's Lecture}

\bi
    \myitem Model checking and diagnostics
	\myitem Variable transformations
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Model selection vs. model checking}

\begin{block}{Assume $ y | \bx = f(\bx) + \epsilon$}
\bi
        \myitem model selection focuses on how you construct $f(\cdot)$;
        \myitem model checking asks whether the $\epsilon$ match the assumed form.
\ei
\end{block}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Model checking: possible challenges}

\begin{block}{Two major areas of concern}
\bi
        \myitem Global lack of fit, or general breakdown of model assumptions
	\bi
		\item Linearity
		\item Unbiased, uncorrelated errors $E(\epsilon | x) = E(\epsilon) = 0$ 
		\item Constant variance $Var(y | x) = Var(\epsilon |x) = \sigma^2$
		\item Independent errors
		\item Normality of errors
	\ei
	\myitem Effect of influential points and outliers
\ei
\end{block}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Model checking: possible solutions and strategies}

\bi
	\myitem Global lack of fit, or general breakdown of model assumptions
	\bi
		\item Residual analysis -- QQ plots, residual plots against fitted values and predictors
		\item Adjusted variable plots
	\ei
	\myitem Effect of influential points and outliers
	\bi
		\item Measure of leverage, influence, outlying-ness
	\ei
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Residual plots: verifying assumptions}

\center Which assumptions are these plots evaluating? 

<<errorModels, echo=FALSE, message=FALSE, fig.height=3>>=
x = sort(runif(200, 0, 10))
error1 = rnorm(200)
error2 = rnorm(200) * x
error3 = rnorm(200, mean = 0, sd = 5)
y1 = 1+2*x + error1
fit1 = lm(y1~x)
y2 = 1+2*x + error2
fit2 = lm(y2~x)
y3 = 1+(x-4)^2 + error3
fit3 = lm(y3~x)
fits <- c(fitted(fit1), fitted(fit2), fitted(fit3))
resids <- c(resid(fit1), resid(fit2), resid(fit3))
dat <- data.frame(fit=rep(1:3, each=200), fitted=fits, residual=resids)
ggplot(dat, aes(fitted, residual)) + geom_point() + geom_smooth(se=FALSE) + facet_wrap(~fit, scales="free") + geom_hline(yintercept=0, color="black")
@

Assumption violations are not often this obvious \\ (but sometimes they are!).

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{QQ-plots for checking Normality of residuals}

\begin{block}{QQ plot defined}
QQ-plot stands for quantile-quantile plot, and is used to compare two distributions. If the two distributions are the same, then each point (which represents a quantile from each distribution) should lie along a line.
\end{block}


\begin{block}{For a single $(x,y)$ point}
\bi
        \myitem $x$ =  a specific quantile for the N(0,1) distribution 
        \myitem $y$ = the same quantile from the sample of data
\ei
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{example: Gaussian or Normal(0,1) distribution}

<<qqnorm, message=FALSE, fig.height=3.5>>=
d1 <- rnorm(1000)
layout(matrix(1:2, nrow=1))
hist(d1, breaks=50, xlim=c(-6, 6))
qqnorm(d1, pch = 19)
qqline(d1)
@

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{example: Student's T-distribution with 6 d.f.}

<<qqt, message=FALSE, fig.height=3.5>>=
d1 <- rt(1000, df=5)
layout(matrix(1:2, nrow=1))
hist(d1, breaks=50, xlim=c(-6, 6))
qqnorm(d1, pch = 19)
qqline(d1)
@

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}[fragile]{example: Truncated Gaussian}

<<qqtrunc, message=FALSE, fig.height=3.5>>=
d1 <- rnorm(1000)
d1 <- subset(d1, abs(d1)<2)
layout(matrix(1:2, nrow=1))
hist(d1, breaks=50, xlim=c(-6, 6))
qqnorm(d1, pch = 19)
qqline(d1)
@

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{QQ-plots for our three fits from earlier}

<<qqplots, echo=FALSE, message=FALSE, fig.height=3>>=
layout(matrix(1:3, nrow=1))
qqnorm(fit1$residuals, pch = 19)
qqline(fit1$residuals)
qqnorm(fit2$residuals, pch = 19)
qqline(fit2$residuals)
qqnorm(fit3$residuals, pch = 19)
qqline(fit3$residuals)
@

<<repeatPlot, fig.height=2, echo=FALSE, message=FALSE>>=
ggplot(dat, aes(fitted, residual)) + geom_point() + geom_smooth(se=FALSE) + facet_wrap(~fit, scales="free") + geom_hline(yintercept=0, color="black")

@


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Typical regression plot: fitted line}

<<lungData, echo=FALSE>>=
data <- read.table("../lecture5-mlr-estimation-formulation/lungc.txt",header=TRUE)
@

%\scriptsize
<<rplot1, fig.height=3, tidy=FALSE, fig.height=4>>=
ggplot(data, aes(crowding, disease)) + 
    geom_point() + geom_smooth(method="lm")
@

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Typical residual plot: fitted vs. residuals}

\scriptsize
<<rplot2, fig.height=3, tidy=FALSE, fig.height=4>>=
slr1 <- lm(disease ~ crowding, data=data)
plot(slr1, which=1)
@

But this is more complicated with MLR: how do we visualize adjusted multivariable relationships?

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Predictor vs. residual plots}

<<rplots3, fig.height=4>>=
library(car)
mlr1 <- lm(disease ~ crowding + education + airqual, data=data)
residualPlots(mlr1, tests=FALSE)
@

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Checking model structure: adjusted variable plots!}

\bi
    \myitem You can plot residuals against each of the predictors, or plot outcomes against predictors, BUT...
    \myitem Keep in mind the MLR uses adjusted relationships; scatterplots don't show that adjustment!
\ei

\vspace{1.5em}

Adjusted variable plots (partial regression plots, added variable plots) can be useful.

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Adjusted (or added) variable plots}

\bi
	\myitem Regress $y$ on everything but $x_{j}$; take residuals $r_{y | -x_{j}}$
	\myitem Regress $x_{j}$ on everything but $x_{j}$; take residuals $r_{x_{j} | -x_{j}}$
	\myitem Regress $r_{y | -x_{j}}$ on $r_{x_{j} | -x_{j}}$; slope of this line will match $\beta_{j}$ in the full MLR
	\myitem Plot of $r_{y | -x_{j}}$ against $r_{x_{j} | -x_{j}}$ shows the ``adjusted" relationship
    \myitem This figure can be used to diagnose violations of linearity in MLR models.
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{AV plots}

<<rplots4, fig.height=4>>=
avPlot(mlr1, variable="airqual")
@

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Model checking: possible solutions}

\bi
        \myitem Global lack of fit, or general breakdown of model assumptions
	\bi
		\item Residual analysis -- QQ plots, residual plots against fitted values and predictors
		\item Adjusted variable plots
	\ei
	\myitem Effect of influential points and outliers
	\bi
		\item Measure of leverage, influence, outlying-ness
	\ei
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Isolated points}

\begin{block}{Points can be isolated in three ways}
\bi
        \myitem Leverage point -- outlier in $x$, measured by hat matrix
	\myitem Outlier -- outlier in $y$, measured by residual
	\myitem Influential point -- a point that largely affects $\bbeta$
	\bi
		\item Deletion influence; $| \hat{\bbeta} - \hat{\bbeta}_{(-i)}|$
		\item Basically, a high-leverage outlier
	\ei
\ei
\end{block}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Quantifying leverage}

We measure leverage (the ``distance" of $\bx_i$ from the distribution of $\bx$) using
$$ h_{ii} = \bx_{i}^{T} (\bX^{T} \bX)^{-1} \bx_{i} $$
where $h_{ii}$ is the $(i,i)^{th}$ entry of the hat matrix. Where, recall

$$ \bH = \bX(\bX^{T}\bX)^{-1}\bX^T $$

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Quantifying Leverage via the Hat Matrix}

Note that 
$$\sum_{i} h_{ii} \stackrel{def}{=} tr(\bH) = p$$
where $p$ is the total number of independent predictors (i.e. $\beta$s) in your model (including a $\beta_0$ if you have one).


\begin{block}{What counts as ``big" leverage?}
\bi
        \myitem Average leverage is $p/n$
	\myitem Typical rules of thumb are $2p/n$ or $3p/n$
	\myitem Leverage plots can be useful as well
\ei
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Example Leverage plot with lung data}

\scriptsize

\scriptsize
<<levPlots1, fig.height=3, tidy=FALSE>>=
mlr <- lm(disease ~ nutrition+ airqual + crowding + smoking, 
          data=data)
hii <- hatvalues(mlr)
x <- 1:length(hii)
qplot(x, hii, geom="point")
@

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Outliers}

\bi
        \myitem When we refer to ``outliers" we typically mean ``points that don't have the same mean structure as the rest of the data"
	\myitem Residuals give an idea of ``outlying-ness", but we need to standardize somehow
	\myitem We can use the fact that $Var(\hat{\epsilon}_{i}) = \sigma^2(1 - h_{ii})$ ...
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Outliers}

The {\it standardized} residual is given by
$$\hat{\epsilon}^{*}_{i} = \frac{\hat{\epsilon}_{i}}{\sqrt{ Var(\hat{\epsilon}_{i})   }} = \frac{\hat{\epsilon}_{i}}{\hat{\sigma}\sqrt{(1 - h_{ii})}} $$

The {\it Studentized} residual is given by
$$t_{i} = \frac{\hat{\epsilon}_{(-i)}}{\hat{\sigma}_{(-i)}\sqrt{(1 - h_{ii})}} = \hat{\epsilon}^{*}_{i}\left(\frac{n-p}{n-p-\hat{\epsilon}^{*2}_{i}} \right)^{1/2}  $$
Studentized residuals follow a $t_{n-p-1}$ distribution.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Influence}

Intuitively, ``influence'' is a combination of outlying-ness and leverage. More specifically, we can measure the ``deletion influence'' of each observation: quantify how much $\hat\bbeta$ changes if an observation is left out.
\bi
	\myitem $|\hat{\bbeta}- \hat{\bbeta}_{(-i)}|$
	\myitem Cook's distance is
	\beqa
		D_i &=& \frac{(\hat\bbeta - \hat\bbeta_{(i)})^T(\bX^T\bX)(\hat\bbeta - \hat\bbeta_{(i)})}{p\hat\sigma^2} \\
			&=& \frac{(\hat \by - \hat \by_{(-i)})^T(\hat \by - \hat \by_{(-i)})}{p\hat\sigma^2} \\
			&=& \frac{1}{p}\hat{\epsilon}_i^2\frac{h_{ii}}{1-h_{ii}}
	\eeqa
\ei

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Handy \texttt{R} functions}

Suppose you fit a linear model in \texttt{R};
\bi
        \myitem \texttt{hatvalues} gives the diagonal elements of the hat matrix $h_{ii}$ (leverages)
	\myitem \texttt{rstandard} gives the standardized residuals
	\myitem \texttt{rstudent} gives the studentized residuals
	\myitem \texttt{cooks.distance} gives the Cook's distances
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Built-in R plots for {\tt lm} objects}

You can also use the {\tt plot.lm()} function to look at leverage, outlying-ness, and influence all together. Recall that
$$D_i = \frac{1}{p}\hat{\epsilon^*}_i^2\frac{h_{ii}}{1-h_{ii}}$$
<<plotlm, fig.height=3.5>>=
plot(mlr, which=5)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Back to the outline}

\bi
    \myitem Model checking and diagnostics
	\myitem Variable transformations
\ei

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{Overview of variable transformations}


\begin{block}{The problems}
\bi
        \myitem Non-linearity between $X$ and $Y$  $\longrightarrow$ transform $X$
        \myitem Skewed distribution of $X$s/points with high leverage $\longrightarrow$ transform $X$
        \myitem Non-constant variance $\longrightarrow$ transform $Y$
\ei
\end{block}

%[More info: KNN Ch 3.9, pp. 129--137]


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Transforming your $X$ variables}

Transforming predictor variables can help with constant-variance non-linear relationships.

<<xtrans, echo=FALSE, fig.height=4, message=FALSE>>=
x <- c(runif(100, 0, 5), runif(100, 0, 400), runif(100, .03, .5))
grp <- rep(1:3, each=100)
y <- x^2*(grp==1) + sqrt(x)*(grp==2) + 1/x*(grp==3) + rnorm(300, sd=2)
dd <- data.frame(x=x, y=y, grp1=grp)
ggplot(dd) + geom_point(aes(x=x, y=y)) + facet_grid(.~grp1, scales="free") + geom_smooth(aes(x, y), span=.9)
@

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Transforming your $X$ variables}


<<xtrans2, echo=FALSE, fig.height=4, message=FALSE>>=
dd$newx <- x^2*(grp==1) + sqrt(x)*(grp==2) + 1/x*(grp==3)
ggplot(dd) + geom_point(aes(x=newx, y=y)) + 
        facet_grid(.~grp1, scales="free") + 
        geom_smooth(aes(newx, y), span=.9)
@

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{$\beta$ interpretations with transformed $X$s}

Transforming predictor variables can help with non-linearities, but can make coefficient interpretations hard.

\begin{block}{Possible solutions}
\bi
        \myitem Interpret $\beta$s qualitatively across a region of interest: ``We found strong evidence for an inverse association, where values of $Y$ decreased inversely proportional to $X$ across the observed range $(a, b)$.
        \myitem Occasionally, a ``one unit change in $X$'' can be meaningful: e.g. $\log_a X$. A one unit change in $\log_a X$ indicates a $a$-fold increase in $X$.
\ei
\end{block}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{$\beta$ interpretations with transformed $X$s}

Transforming predictor variables can help with non-linearities, but can make coefficient interpretations hard.


\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Transforming $Y$s for non-constant variance}

\begin{block}{What to do ...}
\bi
        \myitem Nothing; just use least squares and bootstrap
        \myitem Use weighted LS, GLS (Methods 3?)
        \myitem Use a variance stabilizing transformation
        \myitem Consider a generalized linear model (more soon)
\ei
\end{block}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}[fragile]{Box-Cox Transformations}

Outcome is raised to the $\lambda$ power: 
$$ y^\lambda_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i$$

\bi
        \myitem Estimate $\lambda$, a new parameter, by maximum likelihood.
        \myitem Some well-known choices of $\lambda$: 2, -1, 1/2
        \myitem By definition, when $\lambda=0$, we specify $y^\lambda_i= \log_e y_i$
\ei

% [More detailed info: KNN Ch 3.9, pp. 134--137]

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Today's big ideas}

\bi
	\myitem Model checking
	\myitem Variable transformations
	\myitem Next up: inference about MLR parameters
\ei



\end{frame}



\end{document}
