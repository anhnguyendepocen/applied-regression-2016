%% beamer/knitr slides 
%% for Statistical Modeling and Data Visualization course @ UMass
%% Nicholas Reich: nick [at] schoolph.umass.edu


\documentclass[table]{beamer}


\input{../../slide-includes/standard-knitr-beamer-preamble}

%        The following variables are assumed by the standard preamble:
%        Global variable containing module name:

\title{Multiple Linear Regression: \\ Least squares, colinearity}
%	Global variable containing module shortname:
%		(Currently unused, may be used in future.)
\newcommand{\ModuleShortname}{multRegression}
%	Global variable containing author name:
\author{Nicholas G Reich, Jeff Goldsmith}
%	Global variable containing text of license terms:
\newcommand{\LicenseText}{Made available under the Creative Commons Attribution-ShareAlike 3.0 Unported License: http://creativecommons.org/licenses/by-sa/3.0/deed.en\textunderscore US }
%	Instructor: optional, can leave blank.
%		Recommended format: {Instructor: Jane Doe}
\newcommand{\Instructor}{}
%	Course: optional, can leave blank.
%		Recommended format: {Course: Biostatistics 101}
\newcommand{\Course}{}


\input{../../slide-includes/shortcuts}
\usepackage{bbm}

\hypersetup{colorlinks,linkcolor=,urlcolor=MainColor}


%	******	Document body begins here	**********************

\begin{document}

%	Title page
\begin{frame}[plain]
	\titlepage
\end{frame}

%	******	Everything through the above line must be placed at
%		the top of any TeX file using the statsTeachR standard
%		beamer preamble. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{frame}{Today's topics}

\bi
    \myitem least squares for MLR: geometry, ``hat matrix''
    \myitem collinearity and non-identifiability
    \myitem categorical predictors
\ei

{\bf Example:} predicting respiratory disease severity (``lung'' dataset)


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Multiple linear regression model}

\bi
	\myitem Observe data $(y_i, x_{i1}, \ldots, x_{ip})$ for subjects $1, \ldots, n$. Want to estimate $\beta_0, \beta_1, \ldots, \beta_p$ in the model
	$$ y_i = \beta_0 + \beta_1x_{i1} + \ldots + \beta_px_{ip} + \epsilon_i; \mbox{ } \epsilon_i \stackrel{iid}{\sim} (0,\sigma^2)$$
	
\ei

\begin{block}{Assumptions}
\bi
	\myitem Residuals have mean zero, constant variance, are independent
	\myitem Often assuming linearity
	\myitem Our primary interest will be $E(y | \bx)$
	\myitem Estimation using least squares
\ei
\end{block}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{D\'ej\`a vu: Least squares}

As in simple linear regression, we want to find the $\bbeta$ that minimizes the residual sum of squares.
$$RSS(\bbeta) = \sum_i \epsilon_i ^2 = \epsilon ^T \epsilon$$
\vskip2em

After taking the derivative, setting equal to zero, we obtain:
$$\hat \bbeta = (\bX^{T}\bX)^{-1}\bX ^T \by$$




\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Not so D\'ej\`a vu: the ``Hat matrix''}

\begin{block}{The Hat Matrix}
The hat matrix transforms the observed $\by$ into the fitted values. \\ 
$$\hat{\by} = \bX \hat{\bbeta} = \bX(\bX^{T}\bX)^{-1}\bX^T \by = \bH \by$$
\end{block}

\begin{block}{Some properties of the hat matrix:}
\bi
	\myitem It is a projection matrix: $\bH \bH = \bH$
	\myitem It is symmetric: $\bH^{T} = \bH$
	\myitem The residuals are $\hat{\epsilon} = (\bI - \bH) \by$
	\myitem The inner product of $(\bI - \bH) \by$ and $\bH \by$ is zero (predicted values and residuals are uncorrelated).
\ei
\end{block}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Projection space interpretation}

The hat matrix projects $\by$ onto the column space of $\bX$. Alternatively, minimizing the $RSS(\bbeta)$ is equivalent to minimizing the Euclidean distance between $\by$ and the column space of $\bX$.

\end{frame}

\begin{frame}[t]{Lung Data Example}

99 observations on patients who have sought treatment for the relief of respiratory disease symptoms. 

The variables are:
\bi
    \myitem {\tt disease} measure of disease severity (larger values indicates more serious condition).
    \myitem {\tt education} highest grade completed
    \myitem {\tt crowding} measure of crowding of living quarters (larger values indicate more crowding)
    \myitem {\tt airqual} measure of air quality at place of residence (larger number indicates poorer quality)
    \myitem {\tt nutrition} nutritional status (larger number indicates better nutrition)
    \myitem {\tt smoking} smoking status (1 if smoker, 0 if non-smoker)
\ei

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Lung Data Example}

<<loadData, echo=FALSE>>=
library(ggplot2)
theme_set(theme_bw())
dat <- read.table("lungc.txt", header=TRUE)
opts_chunk$set(size = 'footnotesize')
options(width=60)
@


<<lung-plots, tidy=FALSE, fig.height=4>>=
qplot(crowding, disease, data=dat)
@


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Lung Data Example}

<<lung-plots1, tidy=FALSE, fig.height=4>>=
qplot(education, disease, data=dat)
@


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Lung Data Example}

<<lung-plots2, tidy=FALSE, fig.height=4>>=
qplot(airqual, disease, data=dat)
@


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Lung Data Example}

<<lungMLR, tidy=FALSE>>=
mlr1 <- lm(disease ~ crowding + education + airqual, 
           data=dat, x=TRUE, y=TRUE)
coef(mlr1)
X = mlr1$x
y = mlr1$y
(beta_hat = solve(t(X)%*%X) %*% t(X) %*% y )
@


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Least squares estimates: identifiability}

$$\hat{\bbeta} = \left( \bX^{T} \bX \right)^{-1} \bX^{T} \by$$

\begin{block}{A condition on $\left( \bX^{T} \bX \right)$: must be invertible}
\bi
    \myitem If $\left( \bX^{T} \bX \right)$ is singular, there are infinitely many least squares solutions, making $\hat{\bbeta}$ non-identifiable (can't choose between different solutions)
    \myitem In practice, true {\bf non-identifiability} (there really are infinite solutions) is rare.
    \myitem More common, and perhaps more dangerous, is {\bf collinearity}.
\ei
\end{block}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Causes of non-identifiability}

\bi
	\myitem Can happen if $\bX$ is not of full rank, i.e. the columns of $\bX$ are linearly dependent (for example, including weight in Kg and lb as predictors)
	\myitem Can happen if there are fewer data points than terms in $\bX$: $n < p$ (having 100 predictors and only 50 observations)
	\myitem Generally, the $p \times p$ matrix $\left( \bX^{T} \bX \right)$ is invertible if and only if it has rank $p$.

\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Infinite solutions}

Suppose I fit a model $y_i = \beta_0 + \beta_1 x_{i1} + \epsilon_i$.
\bi
	\myitem I have estimates $\hat{\beta}_0 = 1, \hat{\beta}_1= 2$
	\myitem I put in a new variable $x_2 = x_1$
	\myitem My new model is $y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i$
	\myitem Possible least squares estimates that are equivalent to my first model:
	\bi
		\item $\hat{\beta}_0 = 1, \hat{\beta}_1= 2, \hat{\beta}_2 = 0$
		\item $\hat{\beta}_0 = 1, \hat{\beta}_1= 0, \hat{\beta}_2 = 2$
		\item $\hat{\beta}_0 = 1, \hat{\beta}_1= 1002, \hat{\beta}_2 = -1000$
		\item $\ldots$
	\ei
\ei

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Non-identifiability example: lung data}

<<lungMLRNonIdent, tidy=FALSE>>=
mlr3 <- lm(disease ~ airqual, data=dat)
coef(mlr3)
dat$x2 <- dat$airqual/100
mlr4 <- lm(disease ~ airqual + x2, data=dat, x=TRUE)
coef(mlr4)
X = mlr4$x
solve( t(X) %*% X)
@
 
 
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Non-identifiablity: causes and solutions}

\bi
	\myitem Often due to data coding errors (variable duplication, scale changes)
	\myitem Pretty easy to detect and resolve
	\myitem Can be addressed using {\it penalties} (might come up much later)
	\myitem A bigger problem is near-unidentifiability (collinearity)
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Diagnosing collinearity}

\bi
	\myitem Arises when variables are highly correlated, but not exact duplicates
	\myitem Commonly arises in data (perfect correlation is usually there by mistake)
	\myitem Might exist between several variables, i.e. a linear combination of several variables exists in the data
	\myitem A variety of tools exist (correlation analyses, multiple $R^2$, eigen decompositions)
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Effects of collinearity}

Suppose I fit a model $y_i = \beta_0 + \beta_1 x_{i1} + \epsilon_i$.
\bi
	\myitem I have estimates $\hat{\beta}_0 = 1, \hat{\beta}_1= 2$
	\myitem I put in a new variable $x_2 = x_1 + error$, where $error$ is pretty small
	\myitem My new model is $y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i$
	\myitem Possible least squares estimates that are nearly equivalent to my first model:
	\bi
		\item $\hat{\beta}_0 = 1, \hat{\beta}_1= 2, \hat{\beta}_2 = 0$
		\item $\hat{\beta}_0 = 1, \hat{\beta}_1= 0, \hat{\beta}_2 = 2$
		\item $\hat{\beta}_0 = 1, \hat{\beta}_1= 1002, \hat{\beta}_2 = -1000$
		\item $\ldots$
	\ei
	\myitem A unique solution exists, but it is hard to find
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Effects of collinearity}

\bi
	\myitem Collinearity results in a ``flat" RSS
	\myitem Makes identifying a unique solution difficult
	\myitem Dramatically inflates the variance of LSEs
\ei

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Collinearity example: lung data}

<<lungMLRCollinearity, tidy=FALSE>>=
dat$crowd2 <- dat$crowding + rnorm(nrow(dat), sd=.1)
mlr5 <- lm(disease ~ crowding, data=dat)
summary(mlr5)$coef
mlr6 <- lm(disease ~ crowding + crowd2, data=dat)
summary(mlr6)$coef
@
 
 
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Some take away messages}

\bi
        \myitem Collinearity can (and does) happen, so be careful
	\myitem Often contributes to the problem of variable selection, which we'll touch on later
\ei

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Categorical predictors}

\bi
	\myitem Assume $X$ is a categorical / nominal / factor variable with $k$ levels
	\myitem With only one categorical $X$, we have classic one-way ANOVA design
	\myitem Can't use a single predictor with levels $1, 2, \ldots, K$ -- this has the wrong interpretation
	\myitem Need to create {\it indicator} or {\it dummy} variables
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Indicator variables}

\bi
	\myitem Let $x$ be a categorical variable with $k$ levels (e.g. with $k=3$ ``red'', ``green'', ``blue'').
        \myitem Choose one group as the baseline (e.g. ``red'')
	\myitem Create $(k-1)$ binary terms to include in the model:
\begin{eqnarray*}
        x_{1,i} & = & \mathbbm{1} (x_i=\mbox{``green''}) \\
        x_{2,i} & = & \mathbbm{1} (x_i=\mbox{``blue''}) 
\end{eqnarray*}
	\myitem For a model with no additional predictors, pose the model 
	$$ y_i = \beta_0 + \beta_1 x_{1,i} + \ldots + \beta_{k-1} x_{k-1,i} + \epsilon_{i}$$
	and estimate parameters using least squares
	\myitem Note distinction between {\it predictors} and {\it terms}
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Categorical predictor design matrix}

Which of the following is a ``correct" design matrix for a categorical predictor with 3 levels?

\begin{displaymath}\tiny
	\bX_1 = \left[
		\begin{array}{cccc}
		1 & 1 & 0 & 0 \\
		\vdots & \vdots & \vdots & \vdots \\
		1 & 1 & 0 & 0 \\
		1 & 0 & 1 & 0 \\
		\vdots & \vdots & \vdots & \vdots \\
		1 & 0 & 1 & 0 \\
		1 & 0 & 0 & 1 \\
		\vdots & \vdots & \vdots & \vdots \\
		1 & 0 & 0 & 1 \\
		\end{array}
		\right] \quad \mbox{or} \quad
	\bX_2 = \left[
		\begin{array}{ccc}
		1 & 0 & 0 \\
		\vdots  & \vdots & \vdots \\
		1 & 0 & 0 \\
		1 & 1 & 0 \\
		\vdots & \vdots & \vdots \\
		1 & 1 & 0 \\
		1 & 0 & 1 \\
		\vdots & \vdots & \vdots \\
		1 & 0 & 1 \\
		\end{array}
		\right] \quad \mbox{or} \quad
	\bX_3 = \left[
		\begin{array}{ccc}
		1 & 0 & 0 \\
		\vdots  & \vdots & \vdots \\
		1 & 0 & 0 \\
		0 & 1 & 0 \\
		\vdots & \vdots & \vdots \\
		0 & 1 & 0 \\
		0 & 0 & 1 \\
		\vdots & \vdots & \vdots \\
		0 & 0 & 1 \\
		\end{array}
		\right] \quad
	\end{displaymath}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{ANOVA model interpretation}

Using the model $ y_i = \beta_0 + \beta_1 x_{1,i} + \ldots + \beta_{k-1} x_{k-1,i} + \epsilon_{i}$, interpret

\bigskip

$\beta_0 = $

\vspace{1.5cm}

$\beta_1 = $


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Equivalent model}

Define the model $y_i = \beta_1 x_{i1} + \ldots + \beta_{k} x_{i, k} + \epsilon_{i}$ where there are indicators for each possible group

\bigskip

$\beta_1 = $

\vspace{1.5cm}

$\beta_2 = $


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Categorical predictor example: lung data}

<<lungMLREducCat, tidy=FALSE, message=FALSE, fig.height=4>>=
qplot(factor(education), disease, geom="boxplot", data=dat)
@ 
 
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Categorical predictor example: lung data}

$$ dis_i = \beta_0 + \beta_1 educ_{6,i} + \beta_2 educ_{7,i} + \dots + \beta_{9} educ_{14,i} $$

\small
<<lungMLRCategorical, tidy=FALSE>>=
mlr7 <- lm(disease ~ factor(education), data=dat)
summary(mlr7)$coef
@
 
 
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Categorical predictor releveling}


$$ dis_i = \beta_0 + \beta_1 educ_{5,i} + \beta_2 educ_{6,i} + \beta_1 educ_{7,i} + \beta_2 educ_{9,i} + \dots + \beta_{14} educ_{14,i} $$

\small
<<lungMLRRelevel, tidy=FALSE>>=
dat$educ_new <- relevel(factor(dat$education), ref="8") 
mlr8 <- lm(disease ~ educ_new, data=dat)
summary(mlr8)$coef
@
 
 
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Categorical predictor: no baseline group}

$$ dis_i = \beta_1 educ_{5,i} + \beta_2 educ_{6,i} + \dots + \beta_{14} educ_{14,i} $$

\small
<<lungMLRNoBaseline, tidy=FALSE>>=
mlr9 <- lm(disease ~ factor(education) - 1, data=dat)
summary(mlr9)$coef
@
 
 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}[fragile]{Creating categories using {\tt cut()}}

$$ dis_i = \beta_1 educ_{low,i} + \beta_2 educ_{med,i} + \dots + \beta_{14} educ_{hi,i} $$

\small
<<cut, tidy=FALSE, fig.height=2>>=
dat$educ_3cat <- cut(dat$education, breaks=3, 
                     labels=c("low", "med", "hi"))
mlr10 <- lm(disease ~ educ_3cat - 1, data=dat)
coef(mlr10)
qplot(educ_3cat, disease, geom="boxplot", data=dat)
@
 
 
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Today's big ideas}

\bi
    \myitem least squares geometry, ``hat matrix''
    \myitem dangers of collinearity and non-identifiability
    \myitem categorical predictors
\ei

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%' \begin{frame}[fragile]{Lab}
%' 
%' Analyze the NHANES dataset. Create a model with the outcome variable of cholesterol ({\tt chol}) that estimates relationships with other variables in the dataset.
%' 
%' <<nhanes, warning=FALSE, eval=FALSE>>=
%' library(NHANES)
%' data(NHANES)
%' ?NHANES
%' @
%' 
%' 
%' \end{frame}



\end{document}
